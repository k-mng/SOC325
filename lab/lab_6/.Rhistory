devtools::install_version("RedditExtractoR", version = "2.1.5", repos = "http://cran.us.r-project.org")
## Run this code, to manage packages and install as needed.
# Install pacman
if (!require("pacman")) install.packages("pacman")
# p_load function loads packages if installed, or install then loads otherwise
pacman::p_load(tidyverse,rvest,RedditExtractoR,stm,wordcloud)
theme_set(theme_minimal())
options(scipen = 999)
knitr::opts_chunk$set(error = TRUE)
## Run this code, to manage packages and install as needed.
# Install pacman
if (!require("pacman")) install.packages("pacman")
# p_load function loads packages if installed, or install then loads otherwise
pacman::p_load(tidyverse,rvest,RedditExtractoR,stm,wordcloud)
theme_set(theme_minimal())
options(scipen = 999)
knitr::opts_chunk$set(error = TRUE)
wikipedia_page <-
read_html("https://en.wikipedia.org/wiki/World_Health_Organization_ranking_of_health_systems_in_2000")
wikipedia_page
# CSS selector
section <- html_node(wikipedia_page,
css = "#mw-content-text > div > table.wikitable")
## Run this code, to manage packages and install as needed.
# Install pacman
if (!require("pacman")) install.packages("pacman")
# p_load function loads packages if installed, or install then loads otherwise
pacman::p_load(tidyverse,rvest,RedditExtractoR,stm,wordcloud)
theme_set(theme_minimal())
options(scipen = 999)
knitr::opts_chunk$set(error = TRUE)
# CSS selector
section <- html_node(wikipedia_page,
css = "#mw-content-text > div > table.wikitable")
# CSS selector
section <- html_node(wikipedia_page,
css = "#mw-content-text > div > table.wikitable")
wikipedia_page <-
read_html("https://en.wikipedia.org/wiki/World_Health_Organization_ranking_of_health_systems_in_2000")
## Run this code, to manage packages and install as needed.
# Install pacman
if (!require("pacman")) install.packages("pacman")
# p_load function loads packages if installed, or install then loads otherwise
pacman::p_load(tidyverse,rvest,RedditExtractoR,stm,wordcloud)
theme_set(theme_minimal())
options(scipen = 999)
knitr::opts_chunk$set(error = TRUE)
wikipedia_page <-
read_html("https://en.wikipedia.org/wiki/World_Health_Organization_ranking_of_health_systems_in_2000")
wikipedia_page
# CSS selector
section <- html_node(wikipedia_page,
css = "#mw-content-text > div > table.wikitable")
# XPath
# section <- html_node(wikipedia_page, xpath = '//*[@id="mw-content-text"]/div/table')
section
health_rankings <- html_table(section, fill = T)
health_rankings <- as_tibble(health_rankings, .name_repair = "unique")
health_rankings
reddit_df = get_reddit(subreddit = "QuantifiedSelf", page_threshold = 100)
glimpse(reddit_df)
top_authors =
reddit_df %>%
distinct(author, .keep_all = TRUE) %>%
mutate(author = fct_reorder(author, num_comments)) %>%
arrange(desc(num_comments)) %>%
head(20)
top_authors %>%
ggplot(aes(x=num_comments, y=author)) +
geom_col()
processed <- textProcessor(documents = reddit_df$comment,  stem = FALSE)
processed <- textProcessor(documents = reddit_df$comment,  stem = FALSE)
install.packages("tm")
processed <- textProcessor(documents = reddit_df$comment,  stem = FALSE)
reddit_df$comment
processed <- textProcessor(documents = reddit_df$comment,  stem = FALSE)
processed <- textProcessor(documents = reddit_df$comment,  stem = FALSE)
processed <- textProcessor(documents = reddit_df$comment,  stem = TRUE)
processed <- textProcessor(documents = reddit_df$comment,  stem = FALSE)
out <- prepDocuments(documents = processed$documents, vocab = processed$vocab)
library(tm)
processed <- textProcessor(documents = reddit_df$comment,  stem = FALSE)
out <- prepDocuments(documents = processed$documents, vocab = processed$vocab)
stm5 <-  stm(documents = out$documents, vocab = out$vocab, K = 5, verbose = FALSE)
stm10 <- stm(documents = out$documents, vocab = out$vocab, K = 10, verbose = FALSE)
View(stm5)
par(mfrow=c(1,2))
plot(stm5, n=7, text.cex = 0.7)
plot(stm10, n = 7, text.cex = 0.7)
cloud(stm10, topic = 10)
## Run this code, to manage packages and install as needed.
# Install pacman
if (!require("pacman")) install.packages("pacman")
# p_load function loads packages if installed, or install then loads otherwise
pacman::p_load(tidyverse,rvest,RedditExtractoR,stm,wordcloud)
theme_set(theme_minimal())
options(scipen = 999)
knitr::opts_chunk$set(error = TRUE)
wikipedia_page <-
read_html("https://en.wikipedia.org/wiki/World_Health_Organization_ranking_of_health_systems_in_2000")
wikipedia_page
# CSS selector
section <- html_node(wikipedia_page,
css = "#mw-content-text > div > table.wikitable")
# XPath
# section <- html_node(wikipedia_page, xpath = '//*[@id="mw-content-text"]/div/table')
section
health_rankings <- html_table(section, fill = T)
health_rankings <- as_tibble(health_rankings, .name_repair = "unique")
health_rankings
reddit_df = get_reddit(subreddit = "QuantifiedSelf", page_threshold = 100)
cloud(stm10, topic = 10)
par(mfrow=c(1,2))
plot(stm5, n=7, text.cex = 0.7)
plot(stm10, n = 7, text.cex = 0.7)
stm5 <-  stm(documents = out$documents, vocab = out$vocab, K = 5, verbose = FALSE)
stm10 <- stm(documents = out$documents, vocab = out$vocab, K = 10, verbose = FALSE)
# download the Fitness subreddit
fit_df = get_reddit(subreddit = "Fitness", page_threshold = 100)
cloud(stm10, topic = 10)
# download the Fitness subreddit
fit_df = get_reddit(subreddit = "Fitness", page_threshold = 10)
## Run this code, to manage packages and install as needed.
# Install pacman
if (!require("pacman")) install.packages("pacman")
# p_load function loads packages if installed, or install then loads otherwise
pacman::p_load(tidyverse,rvest,RedditExtractoR,stm,wordcloud)
theme_set(theme_minimal())
options(scipen = 999)
knitr::opts_chunk$set(error = TRUE)
# download the Fitness subreddit
fit_df = get_reddit(subreddit = "Fitness", page_threshold = 10)
# download the Fitness subreddit
fit_df = get_reddit(subreddit = "QuantifiedSelf", page_threshold = 10)
# download the Fitness subreddit
fit_df = get_reddit(subreddit = "TheIchinoseFamily", page_threshold = 10)
# download the Fitness subreddit
fam_df = get_reddit(subreddit = "TheIchinoseFamily", page_threshold = 10)
glimpse(fam_df)
# Look at the top 20 authors
fam_top_authors =
fam_df %>%
distinct(author, .keep_all = TRUE) %>%
mutate(author = fct_reorder(author, num_comments)) %>%
arrange(desc(num_comments)) %>%
head(20)
fam_top_authors %>%
ggplot(aes(x=num_comments, y=author)) +
geom_col() + ggtitle("The Total Number of Comments Made by Each Author") + xlab("# of Comments") + ylab("Username")
# Text Processing the comments
fam_processed <- textProcessor(documents = fam_df$comment,  stem = FALSE)
fam_out <- prepDocuments(documents = fam_processed$documents, vocab = fam_processed$vocab)
# Testing out STM functions
fam_stm5 <-  stm(documents = fam_out$documents, vocab = fam_out$vocab, K = 5, verbose = FALSE)
# I couldn't run the 10, so I'm leaving it here to prove that I know what the code is
fam_stm10 <- stm(documents = fam_out$documents, vocab = fam_out$vocab, K = 10, verbose = FALSE)
# Summarizing the STM functions
par(mfrow=c(1,2))
plot(fam_stm5, n=7, text.cex = 0.7)
plot(fam_stm10, n=7, text.cex = 0.7)
# making the word cloud
cloud(fam_stm10, topic = 5)
# making the word cloud
cloud(fam_stm10, topic = 2)
