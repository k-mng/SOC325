---
title: "Lab 1 Assignment - Review/Introduction to R and Data Structures"
subtitle: "SOC 325: Quantified-Self"
author: "Kim Nguyen"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
---

Write all code in the chunks provided. Complete this `.Rmd` file and knit it into an `.html`. You must upload both files for credit.

Remember to unzip to a real directory before running everything!

# 1. Basics of R and R markdown

## 1.1. Create a vector containing elements 10, 22, 27, 19, 20 and assign it with a name.

```{r}
numbers <- c(10, 22, 27, 19, 20)
```

## 1.2. Use R as a calculator to compute the following values.

```{r}
27*(38-17) 
log(14^7) 
sqrt(436/12) 
```

## 1.3. Run the below code to create a vector. Observe what e contains and use `?seq` to see help of function `seq()`.

```{r}
e <- seq(0, 10, length=5)
e # looks like it lists 5 evenly spaced numbers between 0 and 10
```

Create the following vectors: b = (87, 86, 85, ..., 56)

```{r}
b <- seq(87, 56)
b
```

What is the 19th, 20th, and 21st elements of b?

```{r}
b[c(19,20,21)]
```

## 1.4. Compute the following statistics of b:

a)  sum

```{r eval= TRUE, echo= TRUE}
# I know you technically don't need the = TRUE, but it's been awhile since 
# I've worked with R so I'm just practicing for now!
sum(b)
```

b)  median

```{r eval= TRUE, echo= TRUE}
median(b)
```

c)  standard deviation

```{r eval= TRUE, echo= TRUE}
sd(b)
```

## 1.5. Following the example given in lab1, mix in-line R calculations with text and make reference to vector b. You must use in-line R calculations at least once (e.g. functions like mean(), sd(), max()) and *may not hard-code any numbers referenced in your text*. An example is given below:

The vector of numbers (`b`) that was analyzed was from this list: `r b`.

The average of `b` is `r mean(b)`. The median was `r median(b)`. The standard deviation was `r sd(b)`.

# 2. Research Question (You don't need code for this question)

*For this problem you'll answer some questions to help explore your interests in data science. These are questions that you're interested in. They don't have to be things that you know the answer to and still less new areas of study.*

*However, problem 3 asks you to come up with a 'big data' dataset that you think you might use to answer your question. If you're new to R or not sure about what to do, I encourage you to use the Airbnb data that we'll be using in class. In that case, make sure that your answers to problem 2 relate to the airbnb data.*

## 2.1: What are some areas of interest for you within sociology, big data, and computational social science?

I'm interested in the long-term effects of racial red-lining and how that has affected housing in general. In addition, I have seen that people can plot a map with `R`, so I'm interested in creating one by myself as well.

## 2.2: Provide a link to a dataset which you think intersects with one of your interests. Explain the connection. You can find datasets by doing a google search or by looking [here](http://hadoopilluminated.com/hadoop_illuminated/Public_Bigdata_Sets.html) or [Kaggle](https://www.kaggle.com/)

For this problem, I will be using the class's AirBNB data set. I want to explore and see if there's a correlation between the pricing of an AirBNB and the geography of Seattle, as well as the red-lined parts of Seattle.

Here is the [link](http://insideairbnb.com/get-the-data/) to the AirBNB dataset.

# 3. Import data and identify variables

## 3.1. Import your data into R and output the column names.

I will be working from the Lab directory from my local machine.

```{r warnings = FALSE, message = FALSE,  }
library("R.utils")

# gunzip("../data/listings.csv.gz")
# bc the markdown is in the Lab/lab_1 folder, we have to go back 
airbnb <- read.csv("../../data/lab_1/listings.csv")
```

## 3.2. Use View(), head() or tail() to check your data. What variables does it contain? How many rows are in your data? What is the unit of analysis in your data?

```{r  }
library(knitr)
kable(airbnb[1, ], caption = "First Row of AirBNB dataset")
```

The variables/column names are `r colnames(airbnb)`. There are `r nrow(airbnb)` rows in the data. I will most likely be looking at the latitude and longitude for my personal studies.

## 3.3. Discuss how might some variables serve your research interest as discussed in problem 2 above.

As stated before, the longitudinal and latitudinal coordinates will help me with the coding "dream" that I have, and that is to plot points on a map. In addition, I can use the pricing of the AirBNBs in certain areas to help me determine whether or not redlining has an effect on current housing practices and rentals.

# Problem 4: Piping Hot Variables

## 4.1: Get the data

Go to [Inside Airbnb](http://insideairbnb.com/get-the-data.html) and download the "Detailed Listings" data for Seattle, `listings.csv`. This file has many more variables than the "Summary" file we've been using in class. Put it in a `data/` subfolder in your `hw-02` project folder.

## 4.2: Set up your R environment

```{r message = FALSE, warning= FALSE}
library(tidyverse)
```

## 4.3: Use the data to answer a question

*For how many units does the host live in a different neighborhood from the listing? For how many units does the host live in the same neighborhood as the listing?*

```{r message = FALSE, warning= FALSE,  }

# use columns "neighborhood" and "host_neighborhood"
# using dplyr
library(dplyr)
library(stringr)
place_host <- airbnb %>%
  select(neighbourhood_cleansed, host_neighbourhood, host_id, host_name, host_listings_count) 

place_host <- replace(place_host, place_host == '', NA) # added NA to all blank cells
place_host <- place_host[complete.cases(place_host), ] # removed all rows with NA so that there's no false result

# evaluating whether the host lives in the same city as their listing with a new column
place_host <- place_host %>%
  mutate(same_city = (neighbourhood_cleansed == host_neighbourhood))

```

Despite the thousands of listings in the greater Seattle area, `r nrow(filter(place_host, same_city == TRUE))` listings had hosts that lived in the same area. That means that `r nrow(filter(place_host, same_city == FALSE))` listings had hosts that lived elsewhere. This number does not account for `r (nrow(airbnb)) - (nrow(place_host))` listings that had insufficient information.

## 4.4: Build on your answer

*Building on that work, what is the average number of listings for hosts that live in the same neighborhood as their listing? What's the average for hosts who live in different neighborhoods from their listing?*

```{r message = FALSE,  }

# This variable will contain the TRUE results of hosts who live in AT LEAST ONE of their listings
total_true_listing <- place_host %>%
  # Filter rows so that only TRUE shows
  filter(same_city == TRUE)  %>%
  # Group_by column "host_listings_count" so that it doesn't lose the column
  group_by(host_listings_count) %>%
  # summarize to only unique "host_id" so that there's no extra data when calculating mean
  summarize(unique(host_id))

# This variable will contain the FALSE results for hosts who DO NOT LIVE in the neighborhood of their listings
# Will be a little bit moore difficult because we are removing all unique(host_id) that contain one TRUE
total_false_listing <- place_host %>%
  #group_by so that we don't lose the columns
  group_by(host_id, host_listings_count) %>%
  # summarize to find the number of TRUES in column "same_city"; will create another column
  summarize(sum(same_city)) %>%
  # renamed it for easier typing
  rename("to_remove" = "sum(same_city)") %>%
  # filter so that the "to_remove" column only has results of 0 (because if a host_id has a 0, it means that the host_id does NOT live in any of their listings' neighborhoods)
  filter(to_remove == 0)
```

The average number of listings for hosts that live in *at least one* of their AirBNB listings is `r round(mean(total_true_listing$host_listings_count), digits = 2)`. On contrast, the average number of listings for hosts who *do not live in any of their listings' neighborhood* is `r round(mean(total_false_listing$host_listings_count), digits = 2)`.

## 4.5: Reflect and interpret

**Reflect on your answer to 1.4. What might cause the results you got? How does that connect to the idea that Airbnb might be changing neighborhoods?**

I thought it was really interesting that there are more hosts that run more AirBNBs where they don't live. While this might be the case for now, I did filter down the data so that it would remove any rows with `NA`, as when I first did it, it counted the `NA`s in both columns as `TRUE`, or that the host was living in the same place, which just isn't correct. In addition, there could be more `FALSE`s because it counted "West Seattle" and "Seattle" as different neighborhoods.

With this information, it would be interesting to keep an eye out to where AirBNBs like to pop up; if we don't catch the pattern fast enough, it might take over whole neighborhoods without us even knowing.

# 5. Prepare and Visualize data

## 5.1. Set up your environment

Set up your environment by:

Reading the Airbnb data: There's another new data set in the `data/` folder. This one has almost 10,000 cases and the census data by zipcode. These data are from New York City, not Seattle!

```{r  }
nyc <- read.csv("../../data/lab_1/nyc_airbnb_census_data.csv")
```

We've given you absolute populations and proportions for the racial composition of the zipcode for each listing. We've also made a variable called 'modal_race' which is the race with the largest proportion in that neighborhood.

## 5.2: Turn `price` into a number

`price` includes dollar signs, which means that R interprets it as a character. We want it to be a numeric variable instead. Turn `price` into a numeric variable in the chunk below.

It should be noted that in the CSV file, the price had already been converted to a numeric variable, which can be checked with the code `is.numeric(nyc$price)` (with `nyc` being the variable name containing the data set). However, if I were to do that, I would do the following:

```{r eval = FALSE}

library(readr)
parse_number(nyc$price)

```

## 5.3: Make a scatterplot

**Use a scatter plot to compare how unit prices change with the proportion of a particular race.**

For this scatter plot, we will be comparing the fluctuation of unit prices with the proportions of the White population.

```{r  , warning=FALSE}
library(ggplot2)

# select pricing and prop_white columns
nyc <- nyc %>%
  select(prop_white, price)

# make the scatter plot
nyc_scatter <- nyc %>%
  ggplot(aes(prop_white, # x value
             price)) + geom_point() # y value + scatter POINT

# rename axis and add a title
nyc_scatter <- nyc_scatter + labs(
  title = "Price of AirBNB vs. Prop. of White People in Population",
  y = "Price of AirBNB",
  x = "Proportion of White People in Population"
)

print(nyc_scatter)
```

## 5.4: Make a boxplot

Use the `modal_race` variable to plot a boxplot comparing race and price. You may have to look up how to make a boxplot in `ggplot2`---what geom do you need?

```{r  , warning = FALSE}
nyc_full <- read.csv("../../data/lab_1/nyc_airbnb_census_data.csv")

# select relevant variables
data_boxplot <- nyc_full %>%
  select(modal_race, price)

# remove all NA rows
data_boxplot <- data_boxplot[complete.cases(data_boxplot), ]

# make the boxplot
nyc_boxplot <- data_boxplot %>%
  ggplot(aes(modal_race, # x value
             price, # y value
             )) + geom_boxplot() +
  stat_summary(fun = "mean")

# rename axis and add a title
nyc_boxplot <- nyc_boxplot + labs(
  title = "Price of AirBNB vs. Dominant Race in Area",
  y = "Price of AirBNB",
  x = "Dominant Race in Area"
)

print(nyc_boxplot)
```

As seen from the chart above, it's a bit hard to see the actual *box* of the box plot, so I limited the y axis range to `300` for better clarity.

```{r  , warning = FALSE, message = FALSE}
print(nyc_boxplot + ylim(0,300))
```

## 5.5: Interpret your answer

Looking across the board, it seems as though the average price of AirBNBs is around the same when the modal race is Asian or Black; Hispanics have a slightly lower average price; when the modal race is White, the average price is a lot higher.

Zooming out to look at the full picture, it seems as though whites have a lot more outliers in the higher price range compared to everyone else, even though their total range is less than that of Hispanics (who have an incredible price outlier of \$`r max(data_boxplot$price)`).

This means that, on average, neighborhoods with a white modal race tend to have higher AirBNB prices, with Asian, Black, and Hispanics following behind, respectively.

# 6. Your own data

## 6.1. Research Question

*Looking at the datasets used so far, think about a research question you'd like to investigate (try search about existing studies around your question). What variables do you plan to use to answer your question?*

What is the relationship between the total population of a city in New York and the number of AirBNBs located in that area?

## 6.2. Modification of Data

*What is one way that you have to modify or examine your data to begin to answer your question?*

I am going to have to mutate a new column to count the total number of AirBNBs in a certain city (while filtering out all the `NA` results, of course).

## 6.3. Cleaning Data

*Using the functions we've worked with in class (select, filter, arrange, mutate), plus any others you'd like to use, clean and transform your data set to make it ready for further exploration.*

As you can see, I selected only two columns (`city` and `total_pop`), cleaned it so that it would remove any row without all of the information so that it wouldn't produce any false results, then counted the number of AirBNBs that resided in that city.

```{r message = FALSE,  }
# Select the columns you need to work with 
total_airbnb <- nyc_full %>%
  select(city, total_pop)

# Remove all rows that have NA
total_airbnb <- total_airbnb[complete.cases(total_airbnb), ]

# Make a new table that has total # Airbnb
# I will be doing this by counting the number of occurrences a city has
total_airbnb <- total_airbnb %>%
  group_by(city) %>%
  summarise(num_airbnb = n(),
            total_pop = max(total_pop)) # there were multiple population counts listed for the same city, so I just limited it to the most populated

print(total_airbnb)
```

# Problem 7: Google Trends

## 7.1

*Go to Google Trends and search for "covid-19 vaccine". Look at variations by time and by region in US. What do you observe?*

I set the time period to be January 1st, 2020 and January 1st, 2023. The peak for the phrase "covid-19 vaccine" was around March 7th - March 13th of 2021, with most of it coming from Washington D.C.

When filtering the regions by city, Gaithersburg came up on top. The parts with the most clusters came around Florida and the northeastern part of the U.S.

# Problem 8: Join data frames

In this problem we will use data in the `nycflightdata13` package to perform joining of data frames.

It includes five dataframes, some of which contain missing data (`NA`).

-   `flights`: flights leaving JFK, LGA or EWR in 2013
-   `airlines`: airline abbreviations
-   `airports`: airport metadata
-   `planes`: airplane metadata
-   `weather`: hourly weather data from JFK, LGA and EWR

Note these are **separate data frames**, each needing to be loaded separately using `data()`.

## 8.1. Set up your environment:

a.  Install and load the `nycflights13` package. Load the `tidyverse` package.
b.  Load data sets `flights`, `planes`, `airlines`

```{r  }
library(nycflights13)
(head(flights))
head(planes)
head(airlines)
```

## 8.2 Find data frames

*We'll be looking at who manufactures the planes that flew to Seattle. Which are the two data frames we need to join?*

We will need to join the data frames `flights` and `planes`.

## 8.3. Find common keys

*Take a look at variables contained the two data frames. Which variable(s) should be used as the key to join?*

The common column between both data frames is `tailnum`.

## 8.4. Join the two data frames

```{r  }
# Select specific columns in Flights data set so it's easier to see
flights2 <- flights %>%
  select(tailnum, origin, dest)

planes2 <- planes %>%
  select(tailnum, manufacturer, model)

# Full join both data frames so no information is accidentally lost
sea_manu <- full_join(flights2, planes2, by = "tailnum")

# remove all NA rows
sea_manu <- sea_manu[complete.cases(sea_manu), ]

head(sea_manu)
```

## 8.5. Build on your answer

*For flights with a destination of Seattle, who are the largest manufacturers? Give top five of the manufacturers.* (Check hints if you have troubles)

```{r  }
# Filter data frame to only show dest = SEA
sea_manu <- sea_manu %>%
  filter(dest == "SEA")

# Group_by manufacturer, then count the times that manufacturer occurs. Arrange to show most to least 
sea_manu <- sea_manu %>%
  group_by(manufacturer) %>%
  summarize(num_planes = n()) %>%
  arrange(-num_planes)

print(sea_manu)
```

The largest manufacturers for flights headed towards Seattle is `r sea_manu$manufacturer` (respectively).

## 8.6. Use the data to answer the below questions

*We'd like to know which airlines had the most flights to Seattle from NYC. Which are the two data frames we need to join, and on which key variable(s)?*

The data frames I will be joining is `flights` and `airlines`, with the column `carrier` being the key.

## 8.7.

*Join the two data frames in 8.6 and list the top five airlines.*

```{r  }
library(dplyr)
# select only the columns that I will be using for easier visibility
flights3 <- flights %>%
  select(dest, carrier) # I am not including column "origin" because that lists the AIRPORTS in NYC, NOT the city like I had originally thought

# join the data sets
nyc_sea <- full_join(flights3, airlines, by = "carrier")

# remove all NA
nyc_sea <- nyc_sea[complete.cases(nyc_sea), ]

# filter the results so that dest = SEA
nyc_sea <- nyc_sea %>%
  filter(dest == "SEA")

# Summarize results
nyc_sea <- nyc_sea %>%
  # group by airline name
  group_by(name) %>%
  # count the number of times that airline appears
  summarise(num_airline = n()) %>%
  # arrange so that the top most appears at the top
  arrange(-num_airline)
```

To answer the question, the top five airlines that fly from New York to Seattle is `r nyc_sea$name`. The top airline is `r nyc_sea$name[1]` with `r nyc_sea$num_airline[1]` flights going through.

# Problem 9: Your research question

Think about the research question you have in mind. Plot is a great way to understand patterns, key relationships and uncertainties in a data set. Here we'll ask you to plan about plotting your variables of interest for your research question. Try to think about **3 plots** below:

*For each of the 3 plots, provide:*

A. The purpose of the plot: what do you want people to understand when they see this?

B. The type of plot: what geom functions will you use to present the plot? Why are those the best choices?

C. Limitations/biases: What is missing from this presentation? Could someone get the wrong idea? What can you do to help limit the negative possibilities here?

**Possible Research Question**: As stated before, I am interested in the topic of redlining and the effects it might have on AirBNB pricing. After this assignment, however, I am going to be looking at other areas that redlining might have had an effect as well, such as environmental quality variability. For the three plots, I think it would be best if they worked together to paint a full picture.

### Plot idea 1: Overlayed Scatter Plot (Interactive)

A.  I want people to see the possible relationship between different races and the air quality that they live in. Although I haven't seen the results myself, if there's a correlation between race and air quality, then people will see that there is a pattern that needs to be explored.

B.  As presented in this assignment before, the geom function that will be needed is ` geom_point()` as it is needed for the scatter plot.

C.  This plot, by itself, does not reveal too much about redlining but rather the possible correlation in environmental risks that certain races have to deal with daily.

### Plot idea 2: Map

A.  A digital mapping of counties and where the worst environmental qualities would show where the "worst" place to live is (according to the environment). I think this could be used in tandem with the scatter plot mentioned before, as well as another map showing race distribution

B.  I'm not exactly sure of how to go with this route, but I know that it can be done because of INFO 201. I believe that it also has to do with the latitude and longitudinal coordinates provided in some data frames

C.  The only limitation I can think of is someone drawing the wrong conclusion- they may think that people of certain races *choose* to live in a place that has bad environmental qualities because all three charts still don't prove that redlining has a hand in this. I have tried looking for data sets/maps that showed this, but I will have to continue researching.

### Plot idea 3: Histogram (Interactive)

A.  The purpose of this histogram would show which environmental factors impact the quality of life the most according to different races/class. I want people to understand that people face different things due to race-class differences, and that the factor that could be affecting you is not something that is prioritized in another person's life.

B.  I believe that `geom_bar()` is needed to make a histogram. If I were to go this route, I would do multiple grouped bars to show the different classes for each race, then compare the races to each other

C.  A limitation to this method is that there could be an overload of information for the viewers. This could be helped if I make the charts interactive, but it's still a lot of information to look through and sift.
